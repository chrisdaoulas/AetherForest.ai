# -*- coding: utf-8 -*-
"""Indigenous_Amazon_Deforestation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zpth2gV7TJ9ZK_mOCRwfycwb9JWy-gBi
"""


# Earth Engine Python API
import ee
import os
import time
import pandas as pd
from datetime import date
from datetime import datetime

from google.oauth2 import service_account
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from oauth2client.service_account import ServiceAccountCredentials

os.chdir(os.getcwd()+'\\satellite_data\\scripts')

import folium
from SQL_database import *
from IPFS import *



  

service_account = 'ee-blockchain@ee-blockchain.iam.gserviceaccount.com'
private_key_path =os.path.abspath(os.path.dirname(os.getcwd())+'\\.private-key.json')
credentials = ee.ServiceAccountCredentials(service_account, private_key_path)
ee.Initialize(credentials, project='ee-blockchain')




# Authenticate to the Google Drive of the Service Account
gauth = GoogleAuth()
gauth.LoadClientConfigFile(os.path.dirname(os.getcwd())+'\\secret.json')


#gauth.LoadCredentialsFile(os.environ[u'GOOGLE_APPLICATION_CREDENTIALS'])


#gauth.LocalWebserverAuth()
gauth.LoadCredentialsFile(os.path.dirname(os.getcwd())+'\\mycreds.txt')

if gauth.credentials is None:
    # Authenticate if they're not there
    gauth.settings.update({'get_refresh_token': True})
    gauth.LocalWebserverAuth()
elif gauth.access_token_expired:
    # Refresh them if expired

    gauth.Refresh()
else:
    # Initialize the saved creds
    #gauth.Authorize()
    gauth.LocalWebserverAuth()
# Save the current credentials to a file
gauth.SaveCredentialsFile(os.path.dirname(os.getcwd())+'\\mycreds.txt')
drive = GoogleDrive(gauth)




# Define the URL format used for Earth Engine generated map tiles.
EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'

print('Folium version: ' + folium.__version__)

"""Choose an area of demarcated indigenous land"""

import json
import fiona
import geopandas as gpd
import os



shape = gpd.read_file(os.path.dirname(os.getcwd())+"\\Kayapo\\Amazon_Indigenous-polygon.shp")



js = json.loads(shape.to_json())



kayapo = ee.Geometry(ee.FeatureCollection(js).geometry())

"""Define the forested and non forested samples"""

shape1 = gpd.read_file(os.path.dirname(os.getcwd())+"\\Kayapo\\forest_shp.shp")
js = json.loads(shape1.to_json())
#forest = ee.Geometry(ee.FeatureCollection(js).geometry())
forest = ee.FeatureCollection(js)

shape2 = gpd.read_file(os.path.dirname(os.getcwd())+"\\Kayapo\\non_forest_shp.shp")
js = json.loads(shape2.to_json())
#non_forest = ee.Geometry(ee.FeatureCollection(js).geometry())
non_forest = ee.FeatureCollection(js)

# Create a default map

import geemap
Map = geemap.Map()



# Define the visualization parameters.
vizParams = {
  'bands': ["vis-red", "vis-green", "vis-blue"],
  'min': 0,
  'max': 0.5,
  'gamma': [0.95, 1.1, 1]
}

# Center the map and display the image.

#Map.centerObject(kayapo,8)
#Map.addLayer(kayapo, vizParams, 'false color composite')
#Map.addLayer(forest, vizParams, 'vis-green')
#Map.addLayer(non_forest, vizParams, 'vis-red')

# Display the map
#Map

""""
try:
  import geemap
except ModuleNotFoundError:
  if 'google.colab' in str(get_ipython()):
    print('geemap not found, installing via pip in Google Colab...')
    !pip install geemap --quiet
    import geemap
  else:
    print('geemap not found, please install via conda in your environment')
"""

##Scaling and NDVI filters

def applyScaleFactors(image):
  opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2)
  thermalBands = image.select('SR_B.').multiply(0.0034182).add(149)
  return image.addBands(opticalBands,None,True).addBands(thermalBands,None,True)

def ndviLS(image):
  ndvi = image.normalizedDifference(['SR_B5','SR_B4']).rename('NDVI')
  return image.addBands(ndvi)

##LANDSAT

landsat2015 = ee.ImageCollection('LANDSAT/LC08/C02/T1_L2').filterDate('2015-05-01','2015-08-30').filterBounds(kayapo).filter(ee.Filter.lt('CLOUD_COVER',10)).aside(print).map(ndviLS).map(applyScaleFactors).median().clip(kayapo)


#Map = geemap.Map()


#Map.addLayer(landsat2015.select('NDVI'), {'min': -1, 'max': 1, 'palette': ['blue','white','green']}, 'NDVI2015')

#Map.addLayer(non_forest.draw('red'),{},'No Forest Polygon')
#Map.addLayer(forest.draw('green'),{},'Forest Polygon')
#Map.addLayer(forest, vizParams, 'green')
#Map.addLayer(non_forest, vizParams, 'red')
#Map

bandsLandsat = ['SR_B2','SR_B3','SR_B4','SR_B5','NDVI']

shape1 = gpd.read_file(os.path.dirname(os.getcwd())+"\\Kayapo\\forest_shp.shp")
js = json.loads(shape1.to_json())
#forest = ee.Geometry(ee.FeatureCollection(js).geometry())
forest = ee.FeatureCollection(js)

shape2 = gpd.read_file(os.path.dirname(os.getcwd())+"\\Kayapo\\non_forest_shp.shp")
js = json.loads(shape2.to_json())
#non_forest = ee.Geometry(ee.FeatureCollection(js).geometry())
non_forest = ee.FeatureCollection(js)

#sample created images from landsat image

#non forest

sampledNonForestLandsat = landsat2015.select(bandsLandsat).sampleRegions(**{
   'collection': non_forest, #region to sample over
   'properties': ['landcover'], #the list of properties to copy from each input feature
   'scale': 30 #resolution of the image in meters
})

#forest
sampledForestLandsat = landsat2015.select(bandsLandsat).sampleRegions(**{
   'collection': forest, #region to sample over
   'properties': ['landcover'], #the list of properties to copy from each input feature
   'scale': 30 #resolution of the image in meters
})


############BE CAREFUL WITH THE THRESHOLD, CONSIDER SETTING IT TO 20% AFTERWARDS!!!!!!#############
#80/20 training/test split
threshold = 0.8

trainNonForestLandsat = sampledNonForestLandsat.randomColumn('random').filter(ee.Filter.lte('random',threshold))
testNonForestLandsat = sampledNonForestLandsat.randomColumn('random').filter(ee.Filter.gt('random',threshold))



trainForestLandsat = sampledForestLandsat.randomColumn('random').filter(ee.Filter.lte('random',threshold))
testForestLandsat = sampledForestLandsat.randomColumn('random').filter(ee.Filter.gt('random',threshold))

trainLandsat = trainForestLandsat.merge(trainNonForestLandsat)
testLandsat = testForestLandsat.merge(testNonForestLandsat)

"""Train the random forest classifier with the trained dataset, assigning the desired number of trees,
specifying the class labels and input property
"""

#model
classifier2015 = ee.Classifier.smileRandomForest(10).train(**{ #10 trees cause we are working on a big area and want to be faster
  'features': trainLandsat,
  'classProperty': 'landcover',
  'inputProperties': bandsLandsat
})

"""Get a confusion matrix and compute the accuracy of the model performance"""

#print('Landsat Random Forest Error Matrix: ', classifier2015.confusionMatrix())
#print('Landsat Random Forest Accuracy: ', classifier2015.confusionMatrix().accuracy())
#print("Landsat Random Forest Cohen's Kappa: ", classifier2015.confusionMatrix().kappa())

#model performance looks good. now apply trained model on the landsat and test

"""Predicted values applied on landsat2015 population corresponding Test forest and non-forest samples"""

testingLandsat = testLandsat.classify(classifier2015)

testAccuracyLandsat = testingLandsat.errorMatrix('landcover','classification')

#print('Landsat validation Forest Error Matrix: ', testAccuracyLandsat)
#print('Landsat validation Accuracy: ', testAccuracyLandsat.accuracy())
#print("Landsat validation Cohen's Kappa: ", testAccuracyLandsat.kappa())

"""***TO ADD: CROSS VALIDATION

Predicted values applied on landsat2015 population for  forest and non-forest areas
"""

classified2015= landsat2015.select(bandsLandsat).classify(classifier2015)

"""#External dataset (Optional)
Load the satellite imagery (replace with your own image collection), then load, classify the external dataset, get confusion matrix to see missclassified labels, then compute accuracy and model test performance
"""

imageCollection = ee.ImageCollection("projects/sat-io/open-datasets/landcover/ESRI_Global-LULC_10m_TS").mosaic().clip(landsat2015.geometry())


refPoints_2015 = ee.FeatureCollection('users/vyordanov/Amazon/Amazon2015_refPoints').filterBounds(kayapo)

#Map.addLayer(refPoints_2015.filter(ee.Filter.eq('land_cover',0)).draw('red'),{},'External Validation Points Non Forested 2015',0)
#Map.addLayer(refPoints_2015.filter(ee.Filter.eq('land_cover',1)).draw('green'),{},'External Validation Points Forested 2015',0)

def func_oqa(feat):
  return ee.Feature(feat.geometry(),{'landcover': feat.get('land_cover')})

refPoints_2015 = refPoints_2015.map(func_oqa)

sampleRefPointsLandsat2015 = classified2015.select('classification').sampleRegions(**{
  'collection': refPoints_2015,
  'properties': ['landcover'],
  'scale': 30
})

refAccuracyLandsat = sampleRefPointsLandsat2015.errorMatrix('landcover','classification')

#print('Landsat External validation Forest Error Matrix: ', refAccuracyLandsat)
print('Landsat External validation Accuracy: ', refAccuracyLandsat.accuracy())
#print("Landsat External validation Cohen's Kappa: ", refAccuracyLandsat.kappa())

palette = [
  'red',#non forest
  'green' #forest
]

#Map.addLayer(classified2015, {'min':0, 'max':1, 'palette': palette},'Forest Classification 2015')

#Map

##SENTINEL2##

from datetime import date

print(str(date.today()))

def ndviSE(image):
  ndvi = image.normalizedDifference(['B8','B4']).rename('NDVI')
  return image.addBands(ndvi)

def maskS2cloud (image):

  qa = image.select('QA60'); #Use of QA60 band to mask clouds

  #bits 10: clouds bits 11: cirrus
  cloudBitMask = 1 << 10
  cirrusBitMask = 1 << 11

  #flags should be set to 0 indicating clear conditions
  mask = qa.bitwiseAnd(cloudBitMask).eq(0) \
              .And(qa.bitwiseAnd(cirrusBitMask).eq(0))

  return image.updateMask(mask).divide(10000)

sentinel2023 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \
            .filterDate('2023-05-01', str(date.today())) \
            .filterBounds(kayapo) \
            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE',10)) \
            .aside(print) \
            .map(maskS2cloud) \
            .map(ndviSE) \
            .median() \
            .clip(kayapo);

visParSentinel = {
  'min': 0.0,
  'max': 0.3,
  'bands': ['B4', 'B3', 'B2'],
}

#Map.addLayer(sentinel2023.select('NDVI'), {'min': -1, 'max': 1, 'palette': ['blue','white','green']}, 'NDVI2023')

bandSentinel = ['B2','B3','B4','B8','NDVI']

#sample created images from landsat image

#non forest

sampledNonForestSentinel = sentinel2023.select(bandSentinel).sampleRegions(**{
   'collection': non_forest, #region to sample over
   'properties': ['landcover'], #the list of properties to copy from each input feature
   'scale': 10 #resolution of the image in meters
})


#80/20 training/test split
threshold = 0.8

trainNonForestSentinel = sampledNonForestSentinel.randomColumn('random').filter(ee.Filter.lte('random',threshold))
testNonForestSentinel = sampledNonForestSentinel.randomColumn('random').filter(ee.Filter.gt('random',threshold))

#forest
sampledForestSentinel = sentinel2023.select(bandSentinel).sampleRegions(**{
   'collection': forest, #region to sample over
   'properties': ['landcover'], #the list of properties to copy from each input feature
   'scale': 10 #resolution of the image in meters
})

threshold = 0.8

trainForestSentinel = sampledForestSentinel.randomColumn('random').filter(ee.Filter.lte('random',threshold))
testForestSentinel = sampledForestSentinel.randomColumn('random').filter(ee.Filter.gt('random',threshold))

trainSentinel = trainForestSentinel.merge(trainNonForestSentinel)
testSentinel = testForestSentinel.merge(testNonForestSentinel)

#train the random forest classifier with the trained dataset, assigning the desired number of trees,
#specifying the class labels and input property

#model
classifier2023 = ee.Classifier.smileRandomForest(10).train(**{ #10 trees cause we are working on a big area and want to be faster
  'features': trainSentinel,
  'classProperty': 'landcover',
  'inputProperties': bandSentinel
})

#Get a confusion matrix and compute the accuracy of the model performance

#print('Sentinel Random Forest Error Matrix: ', classifier2023.confusionMatrix())
#print('Sentinel Random Forest Accuracy: ', classifier2023.confusionMatrix().accuracy())
#print("Sentinel Random Forest Cohen's Kappa: ", classifier2023.confusionMatrix().kappa())

#model performance looks good. now apply trained model on the landsat and test

#predicted values applied on landsat2015 population corresponding Test forest and non-forest samples
testingSentinel= testSentinel.classify(classifier2023)

testAccuracySentinel = testingSentinel.errorMatrix('landcover','classification')

#print('Sentinel validation Forest Error Matrix: ', testAccuracySentinel)
#print('Sentinel validation Accuracy: ', testAccuracySentinel.accuracy())
#print("Sentinel validation Cohen's Kappa: ", testAccuracySentinel.kappa())

classified2023= sentinel2023.select(bandSentinel).classify(classifier2023)

"""# External dataset (Optional):
Load the satellite imagery (replace with your own image collection), then load, classify the external dataset, get confusion matrix to see missclassified labels, then compute accuracy and model test performance
"""

refPoints_2023 = ee.FeatureCollection('users/vyordanov/Amazon/Amazon2015_refPoints').filterBounds(kayapo)

#Map.addLayer(refPoints_2023.filter(ee.Filter.eq('land_cover',0)).draw('red'),{},'External Validation Points Non Forested 2023',0)
#Map.addLayer(refPoints_2023.filter(ee.Filter.eq('land_cover',1)).draw('green'),{},'External Validation Points Forested 2023',0)

def func_dkj(feat):
  return ee.Feature(feat.geometry(),{'landcover': feat.get('land_cover')})

refPoints_2023 = refPoints_2023.map(func_dkj)

sampleRefPointsSentinel2023 = classified2023.select('classification').sampleRegions(**{
  'collection': refPoints_2023,
  'properties': ['landcover'],
  'scale': 10
})

refAccuracySentinel = sampleRefPointsSentinel2023.errorMatrix('landcover','classification')

#print('Sentinel External validation Forest Error Matrix: ', refAccuracySentinel)
#print('Sentinel External validation Accuracy: ', refAccuracySentinel.accuracy())
#print("Sentinel External validation Cohen's Kappa: ", refAccuracySentinel.kappa())

palette = [
  'red',#non forest
  'green' #forest
]

#Map.addLayer(classified2023, {min:0,max:1,'palette': palette},'Forest Classification 2023',0);
#Map

"""Application of morphological operator: cleaning noise by applying a smoothing mask in a neighboorhood of k=2"""

classified2015clean = classified2015.focalMode(2)
classified2023clean = classified2023.focalMode(2)

#resample and match resolution, so as to match different scale of satellites used

#turn 30m to 10m for landsat
classified2015_10m = classified2015clean.resample('bicubic').reproject(**{
  'crs': 'EPSG:32721',
  'scale':10
})

#match with sentinel
classified2023_10m = classified2023.reproject(**{
  'crs': 'EPSG:32721',
  'scale':10
})
now= str(datetime.now())
kayapo2015assetId = str('projects/ee-blockchain/assets/classified2015_10m_'+now)[:68].replace(':','-').replace(' ','_')
kayapo2015description= str('classified2015_10m_'+now)[:38].replace(':','-').replace(' ','_')
kayapo2023assetId = str('projects/ee-blockchain/assets/classified2023_10m_'+now)[:68].replace(':','-').replace(' ','_')
kayapo2023description= str('classified2023_10m_'+now)[:38].replace(':','-').replace(' ','_')

ee.batch.Export.image.toAsset(**{
  "image": classified2015_10m,
  "description": kayapo2015description,
  "assetId":kayapo2015assetId,
  "scale":10,
  "crs": 'EPSG:32721',
  "region": kayapo,
  "maxPixels": 1e13
}).start()

time.sleep(900)

ee.batch.Export.image.toAsset(**{
  "image": classified2023_10m,
  "description": kayapo2023description,
  "assetId":kayapo2023assetId,
  "scale":10,
  "crs": 'EPSG:32721',
  "region": kayapo,
  "maxPixels": 1e13
}).start()

time.sleep(1500)

classified2015_10m = ee.Image(kayapo2015assetId)
classified2023_10m = ee.Image(kayapo2023assetId)


"""Compute the difference between two classification maps and add it as new layer, then net forest loss and gain"""

diff = classified2015_10m.subtract(classified2023_10m)
#Map.addLayer(diff,{'min':1, 'mean':0, 'max':1, 'palette':['0000FF','00FF00','FF0000']},'2015 -2023 Difference')

#Compute net forest loss and gain
# 1 Forest- 0 non-forest = 1 Loss # 0 Non forest -1 forest =-1 Gain#

#add the areas representing forest loss only, as a layer
forest_loss=diff.updateMask(diff.eq(1))
#Map.addLayer(forest_loss,{'palette':'FF0000'},'Forest Loss 2015 - 2023')

#add the areas representing forest Gain only, as a layer
forest_gain=diff.updateMask(diff.eq(-1))
#Map.addLayer(forest_gain,{'palette':'00FF00'},'Forest Gain 2015 - 2023')

#compute and print AOI in km2
kayapokm2 = kayapo.area().divide(1000000)

#compute and print the area of the forest loss and gain
areaLoss = forest_loss.multiply(ee.Image.pixelArea().divide(1000000))

statsLoss = areaLoss.reduceRegion(**{
  'reducer': ee.Reducer.sum(),
  'geometry': kayapo,
  'scale': 10,
  'maxPixels': 1e13,
  'tileScale':16
}).getNumber('classification')

#print('Forest Loss  2015 - 2023: ', statsLoss.getInfo(),"km2")

#areaGain = forest_gain.multiply(ee.Image.pixelArea().divide(-1000000))
areaGain = forest_gain.multiply(ee.Image.pixelArea().divide(-1000000))

statsGain = areaGain.reduceRegion(**{
  'reducer': ee.Reducer.sum(),
  'geometry': kayapo,
  'scale': 10,
  'maxPixels': 1e13,
  'tileScale':16
}).getNumber('classification')

#print('Forest Gain  2015 - 2023: ', statsGain.getInfo(),"km2")

"""
Finally, compute and print the relative area of the foreset loss and forest gain
in relation to total area of AOI"""

relLoss = statsLoss.divide(kayapokm2)
#print('Relative Loss: ',relLoss.getInfo()*100,"%")

relGain = statsGain.divide(kayapokm2)
#print('Relative Gain: ',relGain.getInfo()*100,"%")

#print('Net Result: ',relLoss.getInfo()*100 -relGain.getInfo()*100,"%")

#net=relLoss.multiply(100).getInfo() - relGain.multiply(100).getInfo()
net=relLoss.getInfo()*100 -relGain.getInfo()*100

data= {'Net_Change':net, 'Area_Loss':areaLoss.getInfo(),'Area_Gain':areaGain.getInfo(), 'Statistical_Loss':statsLoss.getInfo(),'Statistical_Gain':statsGain.getInfo(),'Realative_Loss':relLoss.getInfo(),'Realtive_Gain':relGain.getInfo()}

with open(os.path.dirname(os.getcwd())+'\\toipfs\\data.txt', 'w') as file:
     file.write(json.dumps(data)) # use `json.loads` to do the reverse
     
###UPLOAD generated FILES





#### DOWNLOAD LOCAL COPIES/ IPFS####
# Export images to local folder
filename1 = os.path.join(os.path.dirname(os.getcwd())+'\\toipfs\\', 'landsat.tif')
filename2 = os.path.join(os.path.dirname(os.getcwd())+'\\toipfs\\', 'sentinel.tif')

#geemap.ee_export_image(classified2023_10m.visualize(**{'bands': ['B4','B3','B2'],'min': 0.0, 'max': 0.3,'palette':['00FFFF', '0000FF']}), filename=filename, scale=90, region=kayapo, file_per_band=False)
geemap.ee_export_image(classified2023_10m.visualize(**{'min':0,'max':1,'palette': palette}), filename=filename1, scale=90, region=kayapo, file_per_band=False)
geemap.ee_export_image(classified2015_10m.visualize(**{'min':0,'max':1,'palette': palette}), filename=filename2, scale=90, region=kayapo, file_per_band=False)



# Alternatively from google drive, Get all files/folders in root
#file_list = drive.ListFile({'q': "'root' in parents and trashed=false",'supportsAllDrives': True,'includeItemsFromAllDrives': True}).GetList()

# gee-images subfolder
#file_list = drive.ListFile({'q': "'1-NR2r1pF03-NRaxIoS1VCEyvB-xdg8QR' in parents and trashed=false",'supportsAllDrives': True,'includeItemsFromAllDrives': True}).GetList()

# Get details of the 'gee_images' folder
#for file in file_list:
 #   print('title: %s, id: %s' % (file['title'], file['id']))
  #  folder_id = file['id']
# Get list of files in 'gee_images' foldersub_file_list = drive.ListFile({'q': "'"+file['id']+"' in parents and trashed=false"}).GetList()

# Export images to drive 
ee.batch.Export.image.toDrive(**{
  "image": classified2015_10m,
  "description": 'classified2015_10m',
  #assetId:'projects/ee-blockchain/classified2015_10m',
  "scale":10,
  "crs": 'EPSG:32721',
  "region": kayapo,
  "maxPixels": 1e13,
  "folder": "gee-images"
}).start()

ee.batch.Export.image.toDrive(**{
  "image": classified2023_10m,
  "description": 'classified2023_10m',
  #assetId:'projects/ee-blockchain/classified2023_10m',
  "scale":10,
  "crs": 'EPSG:32721',
  "region": kayapo,
  "maxPixels": 1e13,
  "folder": "gee-images"
}).start()





time = str(date.today().strftime("%d/%m/%Y"))
#netresult =12.975276442867195

path_to_dir = os.getcwd()+'\\toipfs'
output_filename  = 'latest'
shutil.make_archive(output_filename, 'zip', path_to_dir)


totable = [[net, time, upload_ipfs_pinata('latest.zip')]]
totable = pd.DataFrame(totable, columns=['Rate_of_Deforestation','Time','File_Hash']) 

### ALSO ADD new entry as row to sql and generate only if the genesis table doesn't exist already

try:
    pd_to_sqlDB(totable,table_name='Deforestation_Rate',db_name='defrate.db')
except:
    row_to_sql(totable, table_name='Deforestation_Rate',db_name='defrate.db')

#remove_database_sql2('defrate.db')

# Step 3: Write the SQL query in a string variable
sql_query_string = """
    SELECT * FROM Deforestation_Rate

"""

# Step 4: Exectue the SQL query
result_df = sql_query_to_pd(sql_query_string, db_name='defrate.db')
result_df


# Step 5: Check time duplicates
n_rows=len(result_df)

if n_rows>1:
  timecheck = result_df.iloc[n_rows-1,1]== result_df.iloc[n_rows-2,1]

  if timecheck==True:
      remove_last_sql(table_name='Deforestation_Rate',db_name='defrate.db')
      print("SQL Table Duplicate Removed")
  else:
      print("SQL Table Unique Values Ensured")

"""NOTES: SQL TABLES MUST PERSIST! OTHERWISE CONSIDER MONGODB


"""